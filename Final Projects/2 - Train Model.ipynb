{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " # 1. Setup and Initializations",
   "id": "81c466df7d986b76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T16:30:22.484610Z",
     "start_time": "2025-04-27T16:30:19.433460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import os\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import time\n",
    "from monai.networks.nets import ViT\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# Use DataLoader to load the dataset (json includes the first value as the path to the image and the second value as the label)\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, json_data, label_map, custom_transform=None):\n",
    "        self.data = json_data\n",
    "        self.label_map = label_map\n",
    "        self.transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(), transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])])\n",
    "        if custom_transform:\n",
    "            self.transform = custom_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_sub_path, label = self.data[idx]\n",
    "        img_path = os.path.join(\"data\", img_sub_path)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Transform the images\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(self.label_map[label], dtype=torch.long)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# ------------------\n",
    "# Dataset Functions\n",
    "# ------------------\n",
    "def split_dataset(data_path, train_percent=0.7):\n",
    "    # Split the dataset into train, val, and test sets (4 directories, one for each class. Combine all directories after splitting)\n",
    "    train, val, test = [], [], []\n",
    "    val_percent = ((1-train_percent)) / 2\n",
    "\n",
    "    for dir in os.listdir(data_path):\n",
    "        # Skip non directories\n",
    "        if not os.path.isdir(os.path.join(data_path, dir)):\n",
    "            continue\n",
    "        dir_path = os.path.join(data_path, dir)\n",
    "        dir_json_path = os.path.join(dir_path, \"data.json\")\n",
    "        data = json.load(open(dir_json_path, \"r\"))\n",
    "\n",
    "        # Randomly split the data into train, val, and test sets.\n",
    "        shuffled_data = data.copy()\n",
    "        random.shuffle(shuffled_data)\n",
    "\n",
    "        train_size = int(train_percent * len(shuffled_data))\n",
    "        val_size = int(val_percent * len(shuffled_data))\n",
    "\n",
    "        train_data = shuffled_data[:train_size]\n",
    "        val_data = shuffled_data[train_size:train_size + val_size]\n",
    "        test_data = shuffled_data[train_size + val_size:]\n",
    "\n",
    "        # Add each item in train_data to train\n",
    "        for item in train_data:\n",
    "            train.append(item)\n",
    "\n",
    "        for item in val_data:\n",
    "            val.append(item)\n",
    "\n",
    "        for item in test_data:\n",
    "            test.append(item)\n",
    "\n",
    "    # Save the split data into separate JSON files\n",
    "    with open(os.path.join(data_path, \"train.json\"), \"w\") as f:\n",
    "        json.dump(train, f)\n",
    "    with open(os.path.join(data_path, \"val.json\"), \"w\") as f:\n",
    "        json.dump(val, f)\n",
    "    with open(os.path.join(data_path, \"test.json\"), \"w\") as f:\n",
    "        json.dump(test, f)\n",
    "\n",
    "    # Print length of each set\n",
    "    print(f\"Train set size: {len(train)}\")\n",
    "    print(f\"Validation set size: {len(val)}\")\n",
    "    print(f\"Test set size: {len(test)}\")\n",
    "\n",
    "def get_loaders(base_path, label_map, batch_size):\n",
    "    # Load the dataset\n",
    "    train_json = json.load(open(os.path.join(base_path, \"train.json\"), \"r\"))\n",
    "    val_json = json.load(open(os.path.join(base_path, \"val.json\"), \"r\"))\n",
    "    test_json = json.load(open(os.path.join(base_path, \"test.json\"), \"r\"))\n",
    "\n",
    "    train_dataset= CustomDataset(train_json, label_map=label_map)\n",
    "    val_dataset= CustomDataset(val_json, label_map=label_map)\n",
    "    test_dataset= CustomDataset(test_json, label_map=label_map)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# -------------------\n",
    "# Training Functions\n",
    "# -------------------\n",
    "def train_model(model, model_name, epochs, lr, train_loader, val_loader):\n",
    "    start_time = time.time()\n",
    "    # Prepare learning rate scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    # Move model to be in same place as training\n",
    "    model.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            # Fix for monai models\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation loop\n",
    "        accuracy = test_model(model, val_loader, print_results=False)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} completed. | Loss: {loss.item():.4f} | Accuracy: {accuracy:.2f}% | elapsed time: {time.time() - epoch_start_time:.2f} seconds\")\n",
    "\n",
    "        # TODO: add this back in once hyperparameter tuning is done\n",
    "        # Save the model if it is the best so far\n",
    "        # if epoch == 0 or accuracy > best_accuracy:\n",
    "        #     best_accuracy = accuracy\n",
    "        #     torch.save(model.state_dict(), f'{model_name}_best_model_({accuracy:.2f}).pth')\n",
    "\n",
    "    print(f\"Time elapsed: {time.time() - start_time:.2f} seconds\")\n",
    "    # Save model as pth file\n",
    "    # TODO: consider saving this as a LoRA.\n",
    "    torch.save(model.state_dict(), 'best_vit_model.pth')\n",
    "\n",
    "def test_model(model, test_loader, print_results=True):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            # Fix for monai models\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    if print_results:\n",
    "        print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "def free_up_memory(models:list=None):\n",
    "    if models is not None:\n",
    "        for model in models:\n",
    "            del model # unload models\n",
    "    torch.cuda.empty_cache() # Clear cache state to make each lr run independent\n",
    "    torch.cuda.synchronize() # Wait for all kernels in all streams on a device to finish (may not need, but why not right?)\n",
    "\n",
    "def train_and_test_on_model(model, model_name:str, lrs: list, batch_size: int, epochs: int, train_percent=0.7, data_path=\"data/EDC\", new_dataset_split=True, freeze_layers=[\"\"]):\n",
    "    label_map = {\n",
    "        \"n\": 0, # Normal\n",
    "        \"c\": 1, # Cataract\n",
    "        \"d\": 2, # Diabetic Retinopathy\n",
    "        \"g\": 3  # Glaucoma\n",
    "    }\n",
    "\n",
    "    # Copy of the model to reload between runs\n",
    "    clean_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # Split dataset (can be disabled to test on same set once it's been done once)\n",
    "    if new_dataset_split:\n",
    "        split_dataset(data_path, train_percent=train_percent) # Turn this off to reuse the same dataset split\n",
    "\n",
    "    # Get Loaders\n",
    "    train_loader, val_loader, test_loader = get_loaders(data_path, label_map=label_map, batch_size=batch_size)\n",
    "    first_run = True\n",
    "\n",
    "    # Run for each lr & frozen layer combo\n",
    "    for lr in lrs:\n",
    "        if first_run:\n",
    "            test_model(model, test_loader)\n",
    "            first_run = False\n",
    "        print(\"---------------------\\n\\n---------------------\")\n",
    "        for blocks in freeze_layers:\n",
    "            # Reload the model to reset it\n",
    "            model.load_state_dict(clean_model)\n",
    "            free_up_memory() # Clean up memory before each run\n",
    "\n",
    "            # Freeze subset of blocks\n",
    "            for name, param in model.named_parameters():\n",
    "                if name in blocks:\n",
    "                    param.requires_grad = False\n",
    "                else:\n",
    "                    param.requires_grad = True\n",
    "\n",
    "            print(f\"Training model:{model_name} on learning rate: {lr} | batch size: {batch_size} | epochs: {epochs} | Frozen Layers: {blocks}\")\n",
    "            train_model(model, model_name, epochs, lr, train_loader, val_loader)\n",
    "            acc = test_model(model, test_loader)\n",
    "\n",
    "\n",
    "            hyps = ({\"LR\" : lr}, {\"batch_size\":batch_size}, {\"epochs\": epochs})\n",
    "            modl = ({\"model\": model_name},{\"freeze_layers\": blocks}, {\"accuracy\": acc})\n",
    "            metric = ({\"hyperparameters\": hyps}, {\"model\": modl})\n",
    "            # metric = ({\"LR\":lr}, {\"Accuracy\":acc}, {\"Batch Size\": batch_size})\n",
    "            # append this metric to json file\n",
    "            with open(f'metrics_{model_name}_X.json', 'a') as f:\n",
    "                json.dump(metric, f)\n",
    "                f.write('\\n')\n",
    "\n",
    "    # Clean up\n",
    "    free_up_memory([model])"
   ],
   "id": "3e606d2745c1af1c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derek/anaconda3/envs/AIH/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Load and Test Models",
   "id": "a1bb3fee4a6f2db5"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-27T16:30:33.462486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# These lrs are for generic timm model only\n",
    "lrs = [\n",
    "    #         64 batch                32 batch\n",
    "    #         10 ep       8 ep        8ep\n",
    "    # LRs     run1        run2        run1\n",
    "    1e-4,   # 92.31 |   92.94       91.68\n",
    "    # 2e-4,   # 76.14\n",
    "    # 3e-4,   # 74.73\n",
    "    1e-5,     # 92.94 |   92.78       94.19\n",
    "    2e-5,   # 92.78 |   93.88       93.25\n",
    "    # 3e-5    # 93.41 |   93.41       93.72\n",
    "    ]\n",
    "\n",
    "is_monai = False\n",
    "# lrs = [1e-5]\n",
    "epochs = 7\n",
    "batch_size = 32\n",
    "train_percent = 0.7\n",
    "num_classes = 4\n",
    "model_name = \"UNSET\"\n",
    "\n",
    "free_up_memory()\n",
    "if is_monai:\n",
    "    batch_size = 16\n",
    "    model_name = \"monai\"\n",
    "    model = ViT(\n",
    "        spatial_dims=2,           # Specifies 2D images since MONAI kept doing 3D by default\n",
    "        in_channels=3, img_size=(224,224), patch_size=(16,16),\n",
    "        hidden_size=768,          # Embedding dimension (standard for base ViT)\n",
    "        mlp_dim=3072,             # MLP hidden layer size\n",
    "        num_layers=16,            # Number of transformer layers (ViT-Base)\n",
    "        num_heads=16,             # Number of attention heads\n",
    "        classification=True,      # Enable classification head\n",
    "        num_classes=num_classes,  # Your target classes\n",
    "        dropout_rate=0.1\n",
    "    )\n",
    "\n",
    "    # Load weights\n",
    "    state_dict = torch.load('MODEL_NAME_HERE.pth')\n",
    "    model.load_state_dict(state_dict)\n",
    "else:\n",
    "    model_name = \"timm\"\n",
    "    model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "    model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "\n",
    "    # Blocks 0-11\n",
    "\n",
    "    #        w/o normalization      w/ normalization\n",
    "    # b0-1 = 93.56%                 92.78%\n",
    "    # b0-2 = 93.88%                 %\n",
    "    # b0-3 = 94.35% # best          92.46%\n",
    "    # b0-5 = 94.03%                 93.41%\n",
    "    # b0-8 = 92.78%                 %\n",
    "\n",
    "\n",
    "    # Freeze subset of blocks\n",
    "    # blocks = blocks[:2]\n",
    "    #\n",
    "    # # Freeze all blocks in the blocks list\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if name in blocks:\n",
    "    #         param.requires_grad = False\n",
    "    #     else:\n",
    "    #         param.requires_grad = True\n",
    "    # Freeze all layers except the head (low accuracy 48%)\n",
    "    # for param in model.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    # for param in model.head.parameters():\n",
    "    #     param.requires_grad = True\n",
    "\n",
    "blocks = [\"blocks.0\", \"blocks.1\", \"blocks.2\", \"blocks.3\", \"blocks.4\", \"blocks.5\", \"blocks.6\", \"blocks.7\", \"blocks.8\", \"blocks.9\", \"blocks.10\", \"blocks.11\"]\n",
    "freeze_layers = [\n",
    "    blocks[:2], # first 2 frozen\n",
    "    blocks[:6], # 50% frozen\n",
    "    blocks[:9], # 75% Frozen\n",
    "    # blocks[:12], # 100% frozen\n",
    "]\n",
    "\n",
    "# TODO: remove new_dataset_split once ready for full testing again\n",
    "train_and_test_on_model(model, model_name, lrs=lrs, batch_size=batch_size, epochs=epochs, train_percent=train_percent, new_dataset_split=False, freeze_layers=freeze_layers)\n",
    "\n",
    "# Final cleanup\n",
    "free_up_memory([model])"
   ],
   "id": "307872777ad8de79",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
