{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " # Train The model",
   "id": "81c466df7d986b76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T06:38:44.434895Z",
     "start_time": "2025-04-26T06:38:43.263622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "3e606d2745c1af1c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derek/anaconda3/envs/AIH/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset Functions",
   "id": "fad2bb44e72e6f49"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T06:38:46.475234Z",
     "start_time": "2025-04-26T06:38:46.470507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use DataLoader to load the dataset (json includes the first value as the path to the image and the second value as the label)\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, json_data, label_map, transform):\n",
    "        self.data = json_data\n",
    "        self.transform = transform\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_sub_path, label = self.data[idx]\n",
    "        img_path = os.path.join(\"data\", img_sub_path)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Transform the images\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(self.label_map[label], dtype=torch.long)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def split_dataset(data_path, train_percent=0.7):\n",
    "    # Split the dataset into train, val, and test sets (4 directories, one for each class. Combine all directories after splitting)\n",
    "    train, val, test = [], [], []\n",
    "    val_percent = ((1-train_percent)) / 2\n",
    "\n",
    "    for dir in os.listdir(data_path):\n",
    "        # Skip non directories\n",
    "        if not os.path.isdir(os.path.join(data_path, dir)):\n",
    "            continue\n",
    "        dir_path = os.path.join(data_path, dir)\n",
    "        dir_json_path = os.path.join(dir_path, \"data.json\")\n",
    "        data = json.load(open(dir_json_path, \"r\"))\n",
    "\n",
    "        # Randomly split the data into train, val, and test sets.\n",
    "        shuffled_data = data.copy()\n",
    "        random.shuffle(shuffled_data)\n",
    "\n",
    "        train_size = int(train_percent * len(shuffled_data))\n",
    "        val_size = int(val_percent * len(shuffled_data))\n",
    "\n",
    "        train_data = shuffled_data[:train_size]\n",
    "        val_data = shuffled_data[train_size:train_size + val_size]\n",
    "        test_data = shuffled_data[train_size + val_size:]\n",
    "\n",
    "        # Add each item in train_data to train\n",
    "        for item in train_data:\n",
    "            train.append(item)\n",
    "\n",
    "        for item in val_data:\n",
    "            val.append(item)\n",
    "\n",
    "        for item in test_data:\n",
    "            test.append(item)\n",
    "\n",
    "\n",
    "    # Save the split data into separate JSON files\n",
    "    with open(os.path.join(data_path, \"train.json\"), \"w\") as f:\n",
    "        json.dump(train, f)\n",
    "    with open(os.path.join(data_path, \"val.json\"), \"w\") as f:\n",
    "        json.dump(val, f)\n",
    "    with open(os.path.join(data_path, \"test.json\"), \"w\") as f:\n",
    "        json.dump(test, f)\n",
    "\n",
    "    # Print length of each set\n",
    "    print(f\"Train set size: {len(train)}\")\n",
    "    print(f\"Validation set size: {len(val)}\")\n",
    "    print(f\"Test set size: {len(test)}\")\n",
    "\n",
    "def get_loaders(base_path, label_map, batch_size):\n",
    "    # Load the dataset\n",
    "    train_json = json.load(open(os.path.join(base_path, \"train.json\"), \"r\"))\n",
    "    val_json = json.load(open(os.path.join(base_path, \"val.json\"), \"r\"))\n",
    "    test_json = json.load(open(os.path.join(base_path, \"test.json\"), \"r\"))\n",
    "\n",
    "    transform_rules = [transforms.Resize((224,224)), transforms.ToTensor()] # TODO: add normalization\n",
    "    train_dataset= CustomDataset(train_json, transform=transforms.Compose(transform_rules), label_map=label_map)\n",
    "    val_dataset= CustomDataset(val_json, transform=transforms.Compose(transform_rules), label_map=label_map)\n",
    "    test_dataset= CustomDataset(test_json, transform=transforms.Compose(transform_rules), label_map=label_map)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ],
   "id": "4592a1e4089cee21",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Functions",
   "id": "ddb0b2c07db12fc0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T06:40:43.100033Z",
     "start_time": "2025-04-26T06:40:43.095952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "def train_model(model, model_name, epochs, lr, train_loader, val_loader):\n",
    "    start_time = time.time()\n",
    "    # Prepare learning rate scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    model_stagnation_count = 0\n",
    "\n",
    "    # Move model to be in same place as training\n",
    "    model.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Add accuracy calculation\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} completed. | Loss: {loss.item():.4f} | Accuracy: {accuracy:.2f}% | elapsed time: {time.time() - epoch_start_time:.2f} seconds\")\n",
    "\n",
    "        # Save the model if it is the best so far\n",
    "        if epoch == 0 or accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), f'{model_name}_best_model_({accuracy:.2f}).pth')\n",
    "            # model_stagnation_count = 0\n",
    "        # else:\n",
    "        #     # print(f\"Model not improved. Current best accuracy: {best_accuracy:.2f}%\")\n",
    "        #     model_stagnation_count += 1\n",
    "        #     if model_stagnation_count >= 3:\n",
    "        #         print(f\"Early stopping at epoch {epoch+1} due to no improvement.\")\n",
    "        #         break\n",
    "\n",
    "    print(f\"Time elapsed: {time.time() - start_time:.2f} seconds\")\n",
    "    # ==== SAVE MODEL ====\n",
    "    torch.save(model.state_dict(), 'best_vit_model.pth')\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            # TODO: Verify that this fix for MONAI doesn't break timm\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n"
   ],
   "id": "912f2fd09643a285",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T22:11:18.450908Z",
     "start_time": "2025-04-25T22:09:25.995655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "lrs = [\n",
    "    #       64 batch                32 batch\n",
    "    #       10 ep       8 ep        8ep\n",
    "    # LRs   run1        run2        run1\n",
    "    # 1e-4,   # 92.31 |   92.94       91.68\n",
    "    # 2e-4,   # 76.14\n",
    "    # 3e-4,   # 74.73\n",
    "    1e-5,   # 92.94 |   92.78       94.19\n",
    "    2e-5,   # 92.78 |   93.88       93.25\n",
    "    3e-5]   # 93.41 |   93.41       93.72\n",
    "# lr = 3e-4\n",
    "num_classes = 4\n",
    "batch_size = 32\n",
    "epochs = 1\n",
    "data_path = \"data/EDC/\"\n",
    "label_map = {\n",
    "    \"n\": 0, # Normal\n",
    "    \"c\": 1, # Cataract\n",
    "    \"d\": 2, # Diabetic Retinopathy\n",
    "    \"g\": 3  # Glaucoma\n",
    "}\n",
    "\n",
    "# Split dataset\n",
    "split_dataset(data_path, train_percent=0.7) # Turn this off to reuse the same dataset split\n",
    "\n",
    "# Get Loaders\n",
    "train_loader, val_loader, test_loader = get_loaders(data_path, label_map=label_map, batch_size=batch_size)\n",
    "first_run = True\n",
    "for lr in lrs:\n",
    "    # Load & test generic model\n",
    "    generic_model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "    generic_model.head = nn.Linear(generic_model.head.in_features, num_classes)\n",
    "\n",
    "    if first_run:\n",
    "        test_model(generic_model, test_loader)\n",
    "        first_run = False\n",
    "\n",
    "    # Train generic model\n",
    "    print(f\"\\nTraining with learning rate: {lr}\")\n",
    "    train_model(generic_model, 'timm', epochs, lr, train_loader, val_loader)\n",
    "    acc = test_model(generic_model, test_loader)\n",
    "\n",
    "    metric = ({\"LR\":lr}, {\"Accuracy\":acc}, {\"Batch Size\": batch_size})\n",
    "    # append this metric to json file\n",
    "    with open('metrics.json', 'a') as f:\n",
    "        json.dump(metric, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "    # Clean up\n",
    "    del generic_model # unload model\n",
    "    torch.cuda.empty_cache() # Clear cache state to make each lr run independent\n",
    "    torch.cuda.synchronize() # Wait for all kernels in all streams on a device to finish (may not need, but why not right?)"
   ],
   "id": "1b240d0be37822e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 2949\n",
      "Validation set size: 631\n",
      "Test set size: 637\n",
      "Test Accuracy: 17.11%\n",
      "\n",
      "Training with learning rate: 1e-05\n",
      "Epoch 1/1 completed. | Loss: 0.3356 | Accuracy: 92.71% | elapsed time: 29.65 seconds\n",
      "Time elapsed: 29.87 seconds\n",
      "Test Accuracy: 92.46%\n",
      "\n",
      "Training with learning rate: 2e-05\n",
      "Epoch 1/1 completed. | Loss: 0.0473 | Accuracy: 90.81% | elapsed time: 29.70 seconds\n",
      "Time elapsed: 29.94 seconds\n",
      "Test Accuracy: 89.48%\n",
      "\n",
      "Training with learning rate: 3e-05\n",
      "Epoch 1/1 completed. | Loss: 0.9344 | Accuracy: 91.28% | elapsed time: 29.78 seconds\n",
      "Time elapsed: 30.01 seconds\n",
      "Test Accuracy: 91.68%\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T06:47:11.147785Z",
     "start_time": "2025-04-26T06:47:04.745073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from monai.networks.nets.vit import ViT\n",
    "\n",
    "num_classes = 4\n",
    "lr = 3e-5\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "data_path = \"data/EDC/\"\n",
    "label_map = {\n",
    "    \"n\": 0, # Normal\n",
    "    \"c\": 1, # Cataract\n",
    "    \"d\": 2, # Diabetic Retinopathy\n",
    "    \"g\": 3  # Glaucoma\n",
    "}\n",
    "\n",
    "torch.cuda.empty_cache() # Clear cache state to make each lr run independent\n",
    "torch.cuda.synchronize() # Wait for all kernels in all streams on a device to finish (may not need, but why not right?)\n",
    "\n",
    "# Split dataset\n",
    "split_dataset(data_path, train_percent=0.7) # Turn this off to reuse the same dataset split\n",
    "\n",
    "# Get Loaders\n",
    "train_loader, val_loader, test_loader = get_loaders(data_path, label_map=label_map, batch_size=batch_size)\n",
    "\n",
    "# Monai Test\n",
    "# TODO: Test MONAI model by loading it here then training on top of it.\n",
    "monai_model = ViT(\n",
    "    spatial_dims=2,\n",
    "    in_channels=3,\n",
    "    img_size=(224,224),\n",
    "    patch_size=(16,16),\n",
    "    hidden_size=768,          # Embedding dimension (standard for base ViT)\n",
    "    mlp_dim=3072,             # MLP hidden layer size\n",
    "    num_layers=16,            # Number of transformer layers (ViT-Base)\n",
    "    num_heads=16,             # Number of attention heads\n",
    "    # pos_embed='conv',         # Positional embedding type ('conv' is common in MONAI)\n",
    "    classification=True,      # Enable classification head\n",
    "    num_classes=num_classes,  # Your target classes\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "# monai_model = ViT(in_channels=3, img_size=(224, 224), patch_size=(16, 16), classification=True, num_classes=num_classes) # pos_embed='conv', # dim=768,# depth=12, # heads=12, # mlp_dim=3072, # dropout=0.1)\n",
    "# with torch.no_grad():\n",
    "test_model(monai_model, test_loader)\n",
    "train_model(monai_model, 'monai', epochs, lr, train_loader, val_loader)\n",
    "acc = test_model(monai_model, test_loader)"
   ],
   "id": "6a77c5f0b11b5989",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 2949\n",
      "Validation set size: 631\n",
      "Test set size: 637\n",
      "Test Accuracy: 16.80%\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 43\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# monai_model = ViT(in_channels=3, img_size=(224, 224), patch_size=(16, 16), classification=True, num_classes=num_classes) # pos_embed='conv', # dim=768,# depth=12, # heads=12, # mlp_dim=3072, # dropout=0.1)\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[1;32m     42\u001B[0m test_model(monai_model, test_loader)\n\u001B[0;32m---> 43\u001B[0m train_model(monai_model, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmonai\u001B[39m\u001B[38;5;124m'\u001B[39m, epochs, lr, train_loader, val_loader)\n\u001B[1;32m     44\u001B[0m acc \u001B[38;5;241m=\u001B[39m test_model(monai_model, test_loader)\n",
      "Cell \u001B[0;32mIn[5], line 22\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, model_name, epochs, lr, train_loader, val_loader)\u001B[0m\n\u001B[1;32m     20\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     21\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(images)\n\u001B[0;32m---> 22\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[1;32m     23\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     24\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/anaconda3/envs/AIH/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/AIH/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/anaconda3/envs/AIH/lib/python3.11/site-packages/torch/nn/modules/loss.py:1293\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m   1292\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m-> 1293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mcross_entropy(\n\u001B[1;32m   1294\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m   1295\u001B[0m         target,\n\u001B[1;32m   1296\u001B[0m         weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight,\n\u001B[1;32m   1297\u001B[0m         ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mignore_index,\n\u001B[1;32m   1298\u001B[0m         reduction\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduction,\n\u001B[1;32m   1299\u001B[0m         label_smoothing\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_smoothing,\n\u001B[1;32m   1300\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/AIH/lib/python3.11/site-packages/torch/nn/functional.py:3479\u001B[0m, in \u001B[0;36mcross_entropy\u001B[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[1;32m   3477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3478\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[0;32m-> 3479\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_nn\u001B[38;5;241m.\u001B[39mcross_entropy_loss(\n\u001B[1;32m   3480\u001B[0m     \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m   3481\u001B[0m     target,\n\u001B[1;32m   3482\u001B[0m     weight,\n\u001B[1;32m   3483\u001B[0m     _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction),\n\u001B[1;32m   3484\u001B[0m     ignore_index,\n\u001B[1;32m   3485\u001B[0m     label_smoothing,\n\u001B[1;32m   3486\u001B[0m )\n",
      "\u001B[0;31mTypeError\u001B[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
