{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " # Train The model",
   "id": "81c466df7d986b76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T20:22:55.121861Z",
     "start_time": "2025-04-25T20:22:53.967019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "3e606d2745c1af1c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/derek/anaconda3/envs/AIH/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset Functions",
   "id": "fad2bb44e72e6f49"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T20:22:56.611037Z",
     "start_time": "2025-04-25T20:22:56.606122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use DataLoader to load the dataset (json includes the first value as the path to the image and the second value as the label)\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, json_data, label_map, transform):\n",
    "        self.data = json_data\n",
    "        self.transform = transform\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_sub_path, label = self.data[idx]\n",
    "        img_path = os.path.join(\"data\", img_sub_path)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Transform the images\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Convert label to tensor\n",
    "        label = torch.tensor(self.label_map[label], dtype=torch.long)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def split_dataset(data_path, train_percent=0.7):\n",
    "    # Split the dataset into train, val, and test sets (4 directories, one for each class. Combine all directories after splitting)\n",
    "    train, val, test = [], [], []\n",
    "    val_percent = ((1-train_percent)) / 2\n",
    "\n",
    "    for dir in os.listdir(data_path):\n",
    "        # Skip non directories\n",
    "        if not os.path.isdir(os.path.join(data_path, dir)):\n",
    "            continue\n",
    "        dir_path = os.path.join(data_path, dir)\n",
    "        dir_json_path = os.path.join(dir_path, \"data.json\")\n",
    "        data = json.load(open(dir_json_path, \"r\"))\n",
    "\n",
    "        # Randomly split the data into train, val, and test sets.\n",
    "        shuffled_data = data.copy()\n",
    "        random.shuffle(shuffled_data)\n",
    "\n",
    "        train_size = int(train_percent * len(shuffled_data))\n",
    "        val_size = int(val_percent * len(shuffled_data))\n",
    "\n",
    "        train_data = shuffled_data[:train_size]\n",
    "        val_data = shuffled_data[train_size:train_size + val_size]\n",
    "        test_data = shuffled_data[train_size + val_size:]\n",
    "\n",
    "        # Add each item in train_data to train\n",
    "        for item in train_data:\n",
    "            train.append(item)\n",
    "\n",
    "        for item in val_data:\n",
    "            val.append(item)\n",
    "\n",
    "        for item in test_data:\n",
    "            test.append(item)\n",
    "\n",
    "\n",
    "    # Save the split data into separate JSON files\n",
    "    with open(os.path.join(data_path, \"train.json\"), \"w\") as f:\n",
    "        json.dump(train, f)\n",
    "    with open(os.path.join(data_path, \"val.json\"), \"w\") as f:\n",
    "        json.dump(val, f)\n",
    "    with open(os.path.join(data_path, \"test.json\"), \"w\") as f:\n",
    "        json.dump(test, f)\n",
    "\n",
    "    # Print length of each set\n",
    "    print(f\"Train set size: {len(train)}\")\n",
    "    print(f\"Validation set size: {len(val)}\")\n",
    "    print(f\"Test set size: {len(test)}\")\n",
    "\n",
    "def get_loaders(base_path, label_map, batch_size):\n",
    "    # Load the dataset\n",
    "    train_json = json.load(open(os.path.join(base_path, \"train.json\"), \"r\"))\n",
    "    val_json = json.load(open(os.path.join(base_path, \"val.json\"), \"r\"))\n",
    "    test_json = json.load(open(os.path.join(base_path, \"test.json\"), \"r\"))\n",
    "\n",
    "    transform_rules = [transforms.Resize((224,224)), transforms.ToTensor()] # TODO: add normalization\n",
    "    train_dataset= CustomDataset(train_json, transform=transforms.Compose(transform_rules), label_map=label_map)\n",
    "    val_dataset= CustomDataset(val_json, transform=transforms.Compose(transform_rules), label_map=label_map)\n",
    "    test_dataset= CustomDataset(test_json, transform=transforms.Compose(transform_rules), label_map=label_map)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ],
   "id": "4592a1e4089cee21",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Functions",
   "id": "ddb0b2c07db12fc0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T20:23:01.242547Z",
     "start_time": "2025-04-25T20:23:01.221265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "def train_model(model, model_name, epochs, lr, train_loader, val_loader):\n",
    "    start_time = time.time()\n",
    "    # Prepare learning rate scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    model_stagnation_count = 0\n",
    "\n",
    "    # Move model to be in same place as training\n",
    "    model.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Add accuracy calculation\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} completed. | Loss: {loss.item():.4f} | Accuracy: {accuracy:.2f}% | elapsed time: {time.time() - epoch_start_time:.2f} seconds\")\n",
    "\n",
    "        # Save the model if it is the best so far\n",
    "        if epoch == 0 or accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), f'{model_name}_best_model_({accuracy:.2f}).pth')\n",
    "            # model_stagnation_count = 0\n",
    "        # else:\n",
    "        #     # print(f\"Model not improved. Current best accuracy: {best_accuracy:.2f}%\")\n",
    "        #     model_stagnation_count += 1\n",
    "        #     if model_stagnation_count >= 3:\n",
    "        #         print(f\"Early stopping at epoch {epoch+1} due to no improvement.\")\n",
    "        #         break\n",
    "\n",
    "    print(f\"Time elapsed: {time.time() - start_time:.2f} seconds\")\n",
    "    # ==== SAVE MODEL ====\n",
    "    torch.save(model.state_dict(), 'best_vit_model.pth')\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n"
   ],
   "id": "912f2fd09643a285",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-25T20:31:00.234770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "lrs = [\n",
    "    1e-4,   # 92.31\n",
    "    2e-4,   # 76.14\n",
    "    3e-4,   #\n",
    "    1e-5,   #\n",
    "    2e-5,   #\n",
    "    3e-5]   #\n",
    "# lr = 3e-4\n",
    "num_classes = 4\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "data_path = \"data/EDC/\"\n",
    "label_map = {\n",
    "    \"n\": 0, # Normal\n",
    "    \"c\": 1, # Cataract\n",
    "    \"d\": 2, # Diabetic Retinopathy\n",
    "    \"g\": 3  # Glaucoma\n",
    "}\n",
    "\n",
    "# Split dataset\n",
    "split_dataset(data_path, train_percent=0.7) # Turn this off to reuse the same dataset split\n",
    "\n",
    "# Get Loaders\n",
    "train_loader, val_loader, test_loader = get_loaders(data_path, label_map=label_map, batch_size=batch_size)\n",
    "\n",
    "first_run = True\n",
    "\n",
    "for lr in lrs:\n",
    "    # Load & test generic model\n",
    "    generic_model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "    generic_model.head = nn.Linear(generic_model.head.in_features, num_classes)\n",
    "\n",
    "    if first_run:\n",
    "        test_model(generic_model, test_loader)\n",
    "\n",
    "    # Train generic model\n",
    "    print(f\"\\nTraining with learning rate: {lr}\")\n",
    "    train_model(generic_model, 'timm', epochs, lr, train_loader, val_loader)\n",
    "    acc = test_model(generic_model, test_loader)\n",
    "\n",
    "    metric = (lr, acc)\n",
    "    # append this metric to json file\n",
    "    with open('metrics.json', 'a') as f:\n",
    "        json.dump(metric, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "    # Clean up\n",
    "    del generic_model # unload model\n",
    "    torch.cuda.empty_cache() # Clear cache state to make each lr run independent\n",
    "    torch.cuda.synchronize() # Wait for all kernels in all streams on a device to finish (may not need, but why not right?)\n",
    "\n",
    "\n",
    "# TODO: Test MONAI model by loading it here then training on top of it.\n",
    "# model = ViT(in_channels=3, img_size=(224, 224), patch_size=(16, 16), classification=True, num_classes=_num_classes) # pos_embed='conv', # dim=768,# depth=12, # heads=12, # mlp_dim=3072, # dropout=0.1)"
   ],
   "id": "1b240d0be37822e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 2949\n",
      "Validation set size: 631\n",
      "Test set size: 637\n",
      "Test Accuracy: 26.37%\n",
      "\n",
      "Training with learning rate: 0.0001\n",
      "Epoch 1/10 completed. | Loss: 0.3009 | Accuracy: 83.36% | elapsed time: 28.66 seconds\n",
      "Epoch 2/10 completed. | Loss: 0.2759 | Accuracy: 88.91% | elapsed time: 28.21 seconds\n",
      "Epoch 3/10 completed. | Loss: 0.0077 | Accuracy: 91.28% | elapsed time: 28.30 seconds\n",
      "Epoch 4/10 completed. | Loss: 0.0934 | Accuracy: 86.37% | elapsed time: 28.13 seconds\n",
      "Epoch 5/10 completed. | Loss: 0.0060 | Accuracy: 89.22% | elapsed time: 28.34 seconds\n",
      "Epoch 6/10 completed. | Loss: 0.0003 | Accuracy: 90.97% | elapsed time: 28.55 seconds\n",
      "Epoch 7/10 completed. | Loss: 0.3302 | Accuracy: 88.91% | elapsed time: 28.40 seconds\n",
      "Epoch 8/10 completed. | Loss: 0.0004 | Accuracy: 92.39% | elapsed time: 28.42 seconds\n",
      "Epoch 9/10 completed. | Loss: 0.0000 | Accuracy: 92.71% | elapsed time: 28.42 seconds\n",
      "Epoch 10/10 completed. | Loss: 0.0001 | Accuracy: 92.71% | elapsed time: 28.24 seconds\n",
      "Time elapsed: 286.70 seconds\n",
      "Test Accuracy: 92.31%\n",
      "Test Accuracy: 23.70%\n",
      "\n",
      "Training with learning rate: 0.0002\n",
      "Epoch 1/10 completed. | Loss: 1.3720 | Accuracy: 29.95% | elapsed time: 28.37 seconds\n",
      "Epoch 2/10 completed. | Loss: 0.8035 | Accuracy: 50.24% | elapsed time: 28.47 seconds\n",
      "Epoch 3/10 completed. | Loss: 1.0704 | Accuracy: 42.47% | elapsed time: 28.51 seconds\n",
      "Epoch 4/10 completed. | Loss: 0.6899 | Accuracy: 58.32% | elapsed time: 28.54 seconds\n",
      "Epoch 5/10 completed. | Loss: 1.0965 | Accuracy: 65.93% | elapsed time: 28.39 seconds\n",
      "Epoch 6/10 completed. | Loss: 1.1760 | Accuracy: 70.36% | elapsed time: 28.43 seconds\n",
      "Epoch 7/10 completed. | Loss: 1.0597 | Accuracy: 71.00% | elapsed time: 28.42 seconds\n",
      "Epoch 8/10 completed. | Loss: 0.5407 | Accuracy: 80.51% | elapsed time: 28.41 seconds\n",
      "Epoch 9/10 completed. | Loss: 1.0131 | Accuracy: 78.92% | elapsed time: 28.43 seconds\n",
      "Epoch 10/10 completed. | Loss: 0.5391 | Accuracy: 81.30% | elapsed time: 28.54 seconds\n",
      "Time elapsed: 286.14 seconds\n",
      "Test Accuracy: 76.14%\n",
      "Test Accuracy: 17.90%\n",
      "\n",
      "Training with learning rate: 0.0003\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
